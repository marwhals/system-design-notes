
%    TODO add additional notes from video

\subsection{Challenges with the data lake}

As enterprises start finding more and more use cases to derive value from data, they start to explore the possibility of analysing all these data types with the same set of technologies and tools.

\subsubsection{Problems being solved / Need to be solved}
The problem being solved here is the creation of data silos.
Data analysts are generally running their SQL (rigid structure data) queries on structured data and the data scientists are performing analytics using R or python code in the data lake.
The data analysts are hence missing out on the enormous potential of deriving value from the unstructured data and semi-structured data.
The data scientists struggle to lay their hands on valuable historical information in the structured data sets.
One way to solve this problem is to also load the structured data into the data lake.

\begin{note}
    This is technically possible
\end{note}

\subsubsection{What happens if structured data is also loaded into a data lake?}
The first challenge we encounter is that the files we store in a data lake are immutable
This means you \textit{cannot} modify them in place.
You cannot add rows to the same file or modify the roes inside a file without creating a totally new file.
If you're just processing data in batches, this is not a problem.
You can read the file into a computers memory, modify it and then write it back as a new file with new contents easily.

An organisation does not have just batch processing.
\begin{note}
    There is also the need to store and process data that is continuously streaming in, and data lakes are not natively designed to support streaming data transactions.
\end{note}
ACID- Atomic, Consistent, isolation and durability.
Atomicity means that transactions should either succeed or fail completely, or partial transactions can leave data corrupted and unusable.
Consistency means that a file or a data set might be simultaneously updated by multiple applications.
We need some way of guaranteeing the current state of the file.
Isolation is again a simultaneous operation.
They shouldn't conflict with each other, that is what isolation implies.

\begin{note}
    Important: A data lake does not provide ACID guarantees on data transactions made on it.
\end{note}

Finally: Data analysts who are equal stakeholders in deriving value from datasets are traditionally used to querying data using the SQL language and data.
Data lakes do not have native capability to allow such queries.
How to onboard data analysts to use the data lake and store their data?
Build a \textit{data lake house} on top of a data lake

\subsection{Intoduction to the data lakehouse}

Coined by the founders of Databricks. The same company that also created Apache Spark.

\begin{note}
    The term data lake house refers to the combination of two architectures, data warehouses and data lakes.
\end{note}
By combining these two architectures we are enabling users to store and process large amounts of data with increased reliability, scalability and performance.
This concept is constantly evolving.

In its simplest form you need to have three components.
\begin{itemize}
    \item You need to have a Delta Lake
    \item A SQL query engine
    \item A data catalog.
\end{itemize}

\paragraph{The delta lake}
The delta lake is an open source software that helps maintain a transaction log for all the writes being made on files stored in a data lake.
When you write to a file in a data lake that is being access via the delta lake, what happens is that this write is first logged into a transaction log.
Instead of writing directly to the data file, you make a write first to the transaction log and once the transaction is logged to this file the delta lake updates the data file based on a group of transactions that it is has collected.
This way if multiple applications are making changes to the same data set, the Delta lake can decide the order in which the transaction are performed on the underlying files.

This approach provides a number of benefits.
\begin{itemize}
    \item By writing to the transaction log first the Delta Lake ensures that all changes to the table are safely recorded, even in the event of system failures and errors.
    \item Also aids in data consistency.
    \item The transaction log ensures that all changes to the table are written in a consistent order which helps to avoid conflicts and maintain the integrity of the table.
    \item Last but not least we get transaction atomicity.
    \item By writing the transactions to the log before the data fil, Delta Lake ensures that each transaction is treated as an atomic unit of work, meaning that either all the changes in the transaction are written or none of them.
\end{itemize}

Overall, the write ahead logging approach used by Delta lake helps ensure the reliability and consistency of data in a large scala data lake environment.
The delta lake essentially aids in making the transactions carried out on a data lake become ACID compliant.

\subsubsection{Delta Lake features}
\begin{itemize}
    \item Since the transaction lof aids in versioning, the Delta Lake makes it possible to time travel and rollback data to a certain point in time in the past.
    \item It also has a feature which provides the ability to make tables.
    \item You could even enforce the schema or the structure of data that you require applications to adhere to.
    \item This is in contrast to the principles of a data lake which is to have schema on read.
\end{itemize}

\begin{note}
    The delta lake is also designed to query structured data and hence enforcing the schema is sometimes very beneficial.
\end{note}

Along with the Delta Lake which we have already seen you need to add a SQL query engine to the stack,
This can help you data analysts query the tables created in the Data Lake with a language like SQL and that should help in convincing them to use the data lake instead of the data warehouse to store and process structured data.

\subsubsection{Adding a Data catalog}
Finally, to make sure you have good visibility of the data that is landing inside the data lake, It's important to also include a data catalog to this solution.
\begin{itemize}
    \item now this makes it a complete data lakehouse.
    \item You have the Delta Lake maintaining the log of all transactions.
    \item You have a SQL query engine which will facilitate your data analysts.
    \item You also have a data catalog so that everyone has visibility.
    \item Visibility to the data that is stored in the data lake.
    \item Another Benefit \begin{itemize}
                              \item You can perform, merge, update and delete on the data files stored in the data lake.
                              \item You can even use the data lakehouse to directly store streaming data.
                              \item Not with the data lakehouse, you can store your batch data, your streaming data and this could possibly end all the data silos and collaborate better across the entire organisation.
    \end{itemize}
\end{itemize}

