
%    TODO add additional notes from video

\subsection{Introduction to a data lake}
A data lake serves as a central repository for all data types, structured semi-structured and unstructured.
They can store ANY type of file in this file system.
The difference though is that the data lake usually offers unlimited scalability, whereas single disk drives are limited.
A data lake is best suited for unstructured and semi-structured data.
There is nothing preventing you from storing structured data in a data lake.
When storing structured data in a data lake, you might wish to consider the pros and cons in comparison to storing it in a data warehouse.
The data lake is a concept.
(Authors opinion) There are three underlying principles that need to be fulfilled by any technology or architecture that calls itself a data lake.

\paragraph{First Principle}
The first principle is that the data lake should support \textit{schema on the read}.
This means that data is not checked for a certain structure or consistency during the write process.
The responsibility or the onus of verifying the data in its structure lies exclusively on the reader.
What this effectively does is that it lowers the difficultly in writing data to almost zero.
As a result you are encouraging applications and users to write any data to the data lake. %Data context
Now this does not have a downside.
You data lake might start off as a clean and pristine storage for data and after a while it might end up being messy.
See data governance

\begin{note}
    To collect data you need to lower the barrier by avoiding schemas on the write and rather applying the schema on the read.
    I.e - Write any data to the data lake, check format when reading it.
\end{note}

\paragraph{Second Principle}
Next principle.
In place analytics
Instead of moving data from one database table to another, schema read makes it possible to read the same data file in different ways.
In place analytics avoids the need for making several copies of the same data set and thus saves storage costs and also the time taken to make copies of the same files.

\paragraph{Third Principle?}
ETL vs ELT
\begin{itemize}
    \item When writing data to a data lake in a data pipeline, data is first extracted and loaded into the data lake.
    \item The loading takes place first and its only then that its transformed based, on the requirements by different users for different use cases.
    \item In comparison, data is extracted and transformed before its loaded into a data warehouse.
    \item When you attempt to load the data, we say that a data warehouse enforces schema on write, while we say that a data lake enforces schema on read.
    \item This is because the data warehouse is generally enforcing a certain structure or schema.
\end{itemize}


\begin{note}
    Authors opinion. These three principles are the most essential principles that must be fulfilled for a technology offering to be classed as a data lake.
\end{note}

\subsection{The technology used to build a data lake}
The most commonly used technology for implementing a data lake is cloud storage.
Cloud storage is offered by all cloud vendors and is sole under different brands.

\subsubsection{Common aspects}
Between some technologies, some aspects are common.

\begin{itemize}
    \item All these technologies offer unlimited scalability on demand so you have store terabytes or even petabytes of data without having to buy install or configure any hardware.
    \item You can/could keep loading data and the cloud storage is expected to keep accommodating it.
    \item Usually you would have to pay for what you store and thus if you apply retention policies on data you can have a good grip on the costs associated with storing data for analytics.
    \item These technologies also offer security mechanisms to protect data and prevent unauthorised access to it.
\end{itemize}

\paragraph{Problems with cloud storage}
Problem with cloud storage is that its only available on public cloud:
\begin{itemize}
    \item Makes it challenging for customers who have data which requires a higher level of protection due to regulatory requirements.
    \item For such customers the Hadoop distributed files system has been a popular choice for a long time.
    \item Hadoop is a distributed computing framework which first appeared around 2006.
\end{itemize}

\paragraph{Hadoop}
Hadoop is an open source software ecosystem that allows you to put together a bunch of computers and build a highly reliable distributed system on top of these computers.
You do not need proprietary software nor expensive hardware, and you can easily create a storage repository which could store petabytes of data at a relatively cheap price.
This would be your own data centre.
The problem with hadoop was that inorder to increase the storage capacity you would need to add more computers or servers to the cluster.
This meant that you would inevitably need to add more CPU and memory which is much more expensive that disks making is hard to scale storage without scaling the other two or vice versa.
This is where object storage offerings form popular hardware vendors start to be more appealing.
\begin{note}
    Important note: In order to avoid vendor lock in you can build your own object storage using software like Minio or Ceph.
\end{note}

These vendors provide proprietary hardware and software technology which bundle much denser and faster disks and are scalable independent of the computer power.
Not only that the object storage vendors usually provide the same protocol of access as the storage vendors.
In effect if you need cloud storage on premise then buying object storage technology is a good choice.

\subsubsection{Summary}
These are the technologies that are commonly used to implement a data lake.

\subsection{Cloud Storage terminology - Buckets and blobs}

The data lake is primarily used to load semi-structured and unstructured data.
There is nothing stopping you from loading structured data as well.
All public cloud providers offer some sort of cloud storage as a service.
See AWS, AZURE etc. Data engineering team will provide a reccomendation for the one you wish to work with.

\subsubsection{Cloud storage terminology}
Cloud storage terminology.

\paragraph{Buckets and blobs}
A bucket which is also referred to as a container somtimes as the name suggests rae logical containers used to organised data.

\begin{note}
    Key point is that buckets cannot be nested.
\end{note}

You can have blobs inside a bucket, but you cannot have one bucket inside another.
Bucket names have to be unique globally.
In amazon, not only does it have to be unique in your account but is also has to be unique globally across amazon.
Data engineers typically create a unique random identifier which is added to this logical name for a bucket.
Also, good to know that buckets cannot be renamed.
So if you were to create a bucket and then wish to rename it you would need to create a new bucket and move the contents form the old bucket there.

\begin{note}
    This could prove to be a hassle if you have stored large quantities of data in the bucket.
    Key point: Best thing to do is to write you code to be agnostic to bucket names.
\end{note}

Another important term is a \textit{blob} which also sometimes known as an object.
All the contents inside a bucket are called blobs or objects and these are immutable.
If you were to try and overwrite an object, a new object get created and the old one gets deleted automatically.

You might see objects that look like directories, these are usually called prefixes.
Behind the scenes, a prefix is also an object.
It has some special attributes which make it look like a directory.
%    Add useful diagram here --- how data scientists would incorporate cloud storage into their data science workflows.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
        node distance=2cm,
        box/.style={rectangle, draw, rounded corners, minimum width=3cm, minimum height=1cm, align=center},
        arrow/.style={-Latex}
    ]

        \node[box] (data) {Raw Data};
        \node[box, below=of data] (integration) {Data Integration};
        \node[box, below=of integration] (lake) {Data Lake};
        \node[box, right=of lake] (warehouse) {Data Warehouse};
        \node[box, below=of lake] (analytics) {Analytics};

        \draw[arrow] (data) -- (integration);
        \draw[arrow] (integration) -- (lake);
        \draw[arrow] (lake) -- (warehouse);
        \draw[arrow] (lake) -- (analytics);

    \end{tikzpicture}
    \caption{How data scientists would incorporate cloud storage into their data science workflows}
    \label{fig:cloud storage}
\end{figure}

One use-case: Can read data directly from cloud storage into your data science platform for training models.
Another use case: Could also use cloud storage for storing data that is used for making predictions, and you could even output the predictions into a file that will be stored in the data lake.
This data lake could be on cloud storage.

