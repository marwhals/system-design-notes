%! Author = Marjan
%! Date = 16/06/2025
% Preamble
\documentclass[a4paper, 11pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{hyperref}
\renewcommand{\rmdefault}{phv} % Set the default font family to Helvetica

% Set page margins (A4 paper)
\geometry{top=1in, bottom=1in, left=1in, right=1in}

% Set up the header
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Data Architecture Notes}
%\fancyhead[C]{Your Name}
\fancyhead[R]{\thepage}

% Document
\begin{document}

    % Title Page
    \begin{titlepage}
        \centering
        \vspace*{2in}
        \Huge \textbf{Data Architecture Notes}
%        \vfill
%        \Large Your Name
%        \vfill
%        \Large Date: \today
    \end{titlepage}

    \setcounter{section}{0}

    \newpage

    \tableofcontents
    \newpage

    \section{Introduction}
    Motivations
    - Create models that have the potential to deliver high impact on an organisations performance
    - Many of these models fail to make an impact because you are unable to connect them to the right sources.
    - Connecting to the right data source is important so that the models train on the right data sets that represent the business problem and thereby improving their accuracy and also removing any bias they might display in production.
    - Data engineers and data architects are actually responsible for ensuring models get access to the right data.
    - The reality though is that they require advice and guidance from the data scientists to ensure this does become a reality
    - Knowing data architecture will also help plan projects in such a way.

    Important to plan ahead.
    Allows for notebook dreams to become highly impactful applications.

    Note: Data architecture is outside the core competency of a data scientists roles and responsibility.

%    TODO - Million dollaaaaah slide. LaTeX or mermaid

    \section{Data Type}

    \subsection{Structured data}
    - Structured data typically constitutes only 20\% of the total data volume and organisation produces or has access to.
    -- Many organisations have reached a reasonable level of maturity in dealing with structured data.
    -- Its important to understand how structured data is managed by your data engineering team.

    Structured data can be found in database tables, CSV files and Excel spreadysssss.
    - Organised into rows and columns.
    - The columns contain the features you need to train your model and the rows contain the observations.
    --- Ultimate goal of a data architect or a data engineering team is to extract structured data from the different sources and load it into the data warehouse.
    ----- Since a data warehouse is supposed to have a very accurate information it is not permitted to load data that is unclean or incomplete.
    ----- Hence structured data is first loaded into staging tables where data engineers perform cleansing operation, such as replacing null values in all the columns.
    ----- Staging tables may also be used to join data from different sources and unify them into a single table.
    ----- They may be used to also enrich existing data with information from an external data source.

    All in all the staging tables serve as a playground for the data engineer to transform the data and make it ready for loading the data into the warehouse.
    The data engineer may choose to retain teh staging tables permanently or just generate them dynamically as needed.

    Generally speaking, the data scientists will not have access to the staging tables.
    Once the data is clean unified and enriched, it is read to be loaded into a data warehouse.

    Very important: A data warehouse represents the single source of truth for the entire organisation.

    Example:
    Management of a company wants to know total sales for a month. The expect that business analysts can make a query to the data warehouse and get the answer.
    The data scientist generally has just read access to the data warehouse and is allowed just conditional access to certain tables for exploring the possibility of finding features for data science use cases.
    Once the features are found the data engineer and not the data scientist transfers the required data into what is called a data mart.
    The data mart contains a copy of the data that is available in the data warehouse and is refreshed either on request or as per a predefined schedule.

    Now, the data scientists usually has full access to the data mart and allowed to do further pre-precessing and manipulation of the contents.
    Once the data scientist trains the models.
    Data marts are again used to store the data that is used for making predictions and the predictions themselves are also stored inside the data mart from where it's picked up by applications or just the dashboarding tools.

    \subsection{Unstructured data}
    Most organisations are still learning to extract and derive business value form this type of data.
    The most common types of unstructured data are images, video, audio and text.
    Organisations are starting to find a lot of value in analysing text.
    - There is also computer vision
    - There is text to speech or speech to text.
    --- Can transcribe conversations into text for further analysis.
    --- Natural language processing is trendy.........ChatGPT etc....

    Unstructured data is more than 80\% of the data your organisation produces or has access to it needs a storage medium which iis economical easy to write to.
    This is where the concept of a data lake comes into prominence.
    Unstructured data typically lands into a data lake.
    Once data lands in a data lake, it's available for pre-processing and processing by big data frameworks.

    Unstructured data needs a lot of processing prior to being useful for data science projects.
    Since the volume of this data is huge specialised processing frameworks have evolved over time to manage this challenge.
    -- Typical processing activity includes the scaling of images to allow machine learning algorithms to viably learn from them or extraction if text from audio.
    --- For video footage, video annotation is necessary prior to training models to learn from others.
    --- For NLP use cases then tokenization of text is necessary to produce useful models.

    To perform all these processing tasks, big data frameworks such as Hadoop, Spark and Apache Bean are commonly used.
    All these frameworks are capable of reading data from the data lake, processing it and then writing the results back to the data lake.
    It is important to know that they are capable or reading and writing to the data lake.

    NoSQL
    NoSQL databases are designed to handle a variety of data models and not just tables and rows with columns.
    They can be used to store unstructured data.
    --- This can become very expensive since data has to be stored on expensive disks for performance reasons.
    Best Practice: is to keep unstructured data in a data lake and store the metadata in a NoSQL database.

    This makes it faster to analyse the properties of the images and fetch only those images that fit the criteria.

    \subsection{Semi-structured data}
    This data has some structure but is not confined to just rows and columns.
    You can define the hierarchy and structure using tags and markers.

    --- HTML is also a form of semi-structured data where various elements are separated by tags such as title and body (hehehehehe)
    --- Weblogs are generally produced in the JSON format where JSON stands for JavaScript object notation.

    Data from sensors is also generally produced in JSON and sometimes in the XML format.
    Both these formats have implemented tags or markers which help define a structure and this makes it possible to read them.

    No two datasets will have the exact same structure or even the same hierarchy.

    This makes it easy to write such datasets as developers do not need to confine themselves to a specific structure.

    Becomes challenging for those who read such files most of the time, since the structure needs to be inferred from the files by the reader.
    Key Note: Since storing and processing semi structured data from plain text files makes it compute intensive to process, specialised file formats such as ORC, Parquet and Avro have eveolved.
    -- These file formats are designed to store data in a columnar format which makes it easier and faster to perform analytics on.
    -- Example, if you with to calculate the total temperature from data generated by a sensor it would be easier if the entire temperature was stored in the same file.

    Important: If you are dealing with semi structured data in large volumes its best to store them in on of the big data formats.

    Semi-structured data can be store in the data lake or in a NoSQL database.
    -- Both are suitable for storing semi-structured data.

    Typical flow:
    - First store the semi-structured data in the data lake and then process it using big data frameworks and then store the data in a NoSQL database.
    - Once the big data frameworks have cleaned and formatted the data from the data lake, it could then be stored in the NoSQL database.
    - Storing semi-structured data in a NoSQL database makes it easily accessible by applications and users a like.
    - Hence, the NoSQL database serves as a good serving layer for the semi strcutured data.

    \subsection{Short explaination of JSON and XML structures}
    JSON - Key value pairs.
    XML - Like HTML but not pretty.
    
    \subsection{Semi-Structured data in machine learninig}
    No machine learning framewokrsk accpet data in a semi strcuted format (see ChatGPT)
    Semi-structured data is usually flattened into a structured format prior to being used in model training.
    Flattening is the process of inferring the schema of the dataset and then presenting the required fields in a strucutured format that is in rows and columns.
    Once the data set is flattened you can use the dataset for training models.
    --- It is not essentially a falttened dataset.

    Inference
    - Do predictions on a deployed model.
    Most of the model serving frameworks accept JSON as an input and also generally ouput the predictions in the JSON format.
    Hence no flattening is needed on the inference side of the machine learning workdlow.

%    TODO see example of data being flattened in python

    \section{Data Warehouse}
    
    \subsection{Introduction to Data Warehousing}
    A data warehouse is the single source of truth for the entire organisation.
    A data warehouse is generally used to store information from structured data sources.
    An organisation collects data from its operational databases and organises it inside a data warehouse, in a format you can perform analytics on.
    An operational database is one that holds transactions or data that is written by applications.
    Examples to consider.
    - Could have a web shop which collects its transactions in a sales database.
    - CRM which stores all information in a customer database.............

    Could perform analytics directly on these operational databases.
    However, this will impact the performance of your applications.
    It will be detrimental to your business if you web shop is running slow, just because some data scientist was querying the data to explore the available features.
    It's also the case that operational database are designed to store transactions and are not easy to query for analytical purposes. (Pretty key point)
    Operational databases are designed to be written to, whereas analytical databases need to provide more read performance.
    Analytical databases perform better when the data is stored in a columnar format, whereas operational databases are designed for storing transactions in rows.

    Very key point.
    Makes a lot of sense for any organisation to extract data from all its operational databases and store it in a data warehouse,
    and store it in a way that makes it easy for querying by analysts and data scientists alike.

    Companies also enrich their data warehouse with data from external sources.
    Combine real time data from the internet with operational data.

    \subsection{Datawarehousing for data scientists}

    A data warehouse should have an almost perfect view of the entire organisations structured data estate.
    Good place to find data to train you machine learning models.
    Note: If you are struggling to improve the accuracy of your model due to lack of feature, it is working looking into the data warehouse to find additional data.
    Note: Generally speaking users only get read access to the data warehouse.
    Also, common to impose restrictions on the visibility of data....don't expect all access from day one.

    Most organisations implement a data catalog to provide you with a preview of the data products available in your organisation.
    A modern enterprise will not let the data scientists query the data directly but instead ask them to query the data directly, but instead ask them to explore the data sets via the data catalog.
    -- Reality is that most enterprises are not there yet so data warehouse skills could be in demand........

    Once the data scientist finds the data needed for training models, he or she will request data engineering team to transfer it to a data mart or even to a feature store.
    This could be a one time transfer ot a transfer set on a schedule.
    Example: a data scientist might request the data engineer to schedule the transfer every week so that fresh data is available on a weekly basis for model training.
    -- The data scientists then uses a data science platform or his/her own development environment to engineer new features or pre-process the data for the purposes of model training.

    From a data architecture perspective, its important to note that all these tasks are carried out inside the data mart or a feature store as opposed to inside a data warehouse.
    Once a model is trained, the inference or predictions are also carried out against the data mart or a feature store and not the data warehouse directly.

    Data is extracted on a set schedule, which is then sent to the model for predictions.
    The predictions are also stored in a data mart which then serves dashboards or applications that utilise the predictive capabilities.
    Over of what data warehousing typically means and also how data scientists would typically interact with it.

    \subsection{Cloud datawarehousing}

    Old way. Data centres. Hardware bundled with proprietary software.
    Data centres were previously filled with these expensive stacks.
    On premise data warehouses did perform very well, and they did help organisations derive massive value.
    Companies need to make heavy upfront investments to buy, install configure and to maintain these systems and to maintain these systems.
    -- There is also no flexibility either.
    -- Had to size the data warehouse for your heavy month end or quarterly reporting jobs.
    -- This meant that hardware was unused for the rest of the time.
    Can trace back the origin of cloud data warehousing.
    Cloud data warehouses are hosted natively on public cloud infrastructure as opposed to hardware hosted on premise.
    -- By this, compute memory and storage managed by the cloud providers.
    -- Moreover, they are usually fully managed and hence you don't need staff to manage this infrastructure or perform software updates for that matter.
    All this is managed for you as a bundled service.
    Means you can focus on the more important takes, such as loading the data warehouse and extracting the data out of it.
    What differentiates cloud data warehousing from traditional data warehousing is, number one, the decoupling of compute and storage, which makes it flexible to have different sizes of data warehouses for different needs and that too is on demand.
    -- Is you have to run a monthly reporting on your data, you can spin up a large data warehouse.
    -- Once that report is done, you can terminate the instance or scale it down to a smaller instance for your day-to-day needs.

    Snowflake is a popular cloud data warehousing technology.
    Uses t-shirt sizing for its sizing in order to make it easier for users to choose the appropriate size for the appropriate time.
    Modern cloud data warehousing also support semi-structured data natively.
    Software vendors are trying to support more and more data types making it easy for customers to have just one technology for all data types.
    Technology like snowflake, also strives to cater to multiple use cases.
    Instead of using a separate technology for configuring a data mart, feature store and a data warehouse.
    The flexibility snowflake provides allows you to use it for all three purposes.

    While specialised feature stores do have an edge, snowflake offers to integrate with them and thus offers the best of both worlds.

    \section{Data Lake}

    \subsection{Introduction to a data lake}
    A data lake serves as a central repository for all data types, structured semi-structured and unstructured.
    Can store ANY type of file in this file system.
    The difference though is that the data lake usually offers unlimited scalability, whereas single disk drives are limited.
    Data lake is best suited for unstructured and semi-structured data.
    There is nothing preventing you from storing structured data in a data lake.
    When storing structured data in a data lake, you might wish to consider the pros and cons in comparison to storing it in a data warehouse.
    The data lake is a concept (opinion).
    There are three underlying principles that need to be fulfilled by any technology or architecture that calls itself a data lake.
    The first principle is that the data lake should support schema on th read.
    --- This means that data is not check for a certain structure or consistency during the write process.
    --- The responsibility or the onus of verifying the data in its structure lies exclusively on the reader.
    --- What this effectively does is that it lowers the difficultly in writing data to almost zero.
    ------ As a result you are encouraging applications and users to write any data to the data lake.
    ------ Now this does not have a downside.
    ------ You data lake might start off as a clean and pristine storage for data and after a while it might end up being messy. See data governance

    To collect data you need to lower the barrier by avoiding schemas on the write and rather applying the schema on the read.
    Next principle.
    --- In place analytics
    ------ Instead of moving data from one database table to another, schema read makes it possible to read the same data file in different ways.
    ------ In place analytics avoids the need for making several copies of the same data set and thus saves storage costs and also the time taken to make copies of the same files.

    ETL vs ELT
    ---- When writing data to a data lake in a data pipeline, data is first extracted and loaded into the data lake.
    -- The loading takes place first and its only then that its transformed based on the requirements by different users for different use cases.
    -- In comparison, data is extracted and transformed before its loaded into a data warehouse.
    -- This is because the data warehouse is generally enforcing a certain structure or schema.
    --- When you attempt to load the data, we say that a data warehouse enforces schema on write, while we say that a data lake enforces schema on read.

    Authors opinion.
    These three principles are the most essential principles that must be fulfilled for a technology offering to be classed as a data lake.

    \subsection{The technology used to build a data lake}
    Most commonly used technology for implementing a data lake is cloud storage.
    Cloud storage is offered by all cloud vendors and is sole under different brands.

    Between some technologies some aspects are common.
    - All these technologies offer unlimited scalability on demand so you have store terabytes or even petabytes of data without having to buy install or configure any hardware.
    - Could keep loading data and the cloud storage is expected to keep accommodating it.
    - Usually you would have to pay for what you store adn thus if you apply retention policies on data you can have a good grip on the costs associated with storing data for analytics.
    --- These technologies also offer security mechanisms to protect data and prevent unauthorised access to it.

    Problem with cloud storage is that its only available on public cloud.
    --- Makes it challenging for customers who have data which requires a higher level of protection due to regulatory requirements.
    --- For such customers the Hadoop distributed files system has been a popular choice for a long time
    ----- Hadoop is a distributed computing framework which first appeared around 2006

    Hadoop
    - Hadoop is an open source software ecosystem that allows you to put together a bunch of computers and build a highly reliable distributed system on top of these computers.
    --- reason for its popularity ^^^
    - Do not need proprietary software nor expensive hardware, and you can easily create a storage repository which could store petabytes of data at a relatively cheap price.
    --- Would be your own data centre.
    -- The problem with hadoop was that inorder to increase the storage capacity you would need to add more computers or servers to the cluster.
    -- This meant that you would inevitably need to add more CPU and memory which is much more expensive that disks making is hard to scala storage without scaling the other two or vice versa.
    ------ This is where object storage offerings form popular hardware vendors start to be more appealing.
    Important note: In order to avoid vendor lock in you can build your own object storage  using software like Minio or Ceph
    -- These vendors I mentioned before provide proprietary hardware and software technology which bundle much denser and faster disks and are scalable independent of the computer power.
    -- Not only that the object storage vendors usually provide the same protocol of access as the storage vendors.
    -- In effect if you need cloud storage on premise then buying object storage technology is a good choice.

    These are the technologies that are commonly used to implement a data lake.
    
    \subsection{Cloud Storage terminology - Buckets and blobs}

    The data lake is primarily used to load semi-structured and unstructured data.
    - Nothing stopping you from loading structured data as well.
    - All public cloud providers offer some sort of cloud storage as a service.
    -- See AWS, AZURE etc. Data engineering team will provide a reccomendation for the one you wish to work with.
    Cloud storage terminology.
    - Buckets and blobs
    A bucket which is also referred to as a container somtimes as the name suggests rae logical containers used to organised data.
    -- Key point is that buckets cannot be nested.
    You can have blobs inside a bucket, but you cannot have one bucket inside another.
    -- Bucket names have to be unique globally.
    ---- In amazon, not only does it have to be unique in your account but is also has to be unique globally across amazon.
    Data engineers typically create a unique random identifier which is added to this logical name for a bucket.
    Also, good to know that buckets cannot be renamed.
    -- So if you were to create a bucket and then wish to rename it you would need to create a new bucket and move the contents form the old bucket there.
    ---- This could prove to be a hassle if you have stored large quantities of data in the bucket.
    Key point: Best thing to do is to write you code to be agnostic to bucket names.

    Another important term is a blob which also sometimes known as an object.
    All the contents inside a bucket are called blobs or objects and these are immutable.
    If you were to try and overwrite an object, a new object get created and the old one gets deleted automatically.

    You might see objects that look like directories, these are usually called prefixes.
    - Behind the scene a prefix is also an object.
    - Has some special attributed which make it look like a directory.
%    Add useful diagram here --- how data scientists would incorporate cloud storage into their data science workflows.
    One use-case
    - Can read data directly from cloud storage into your data science platform for training models.
    Another use case
    - Could also use cloud storage for storing data that is used for making predictions, and you could even output the predictions into a file that will be stored in the data lake.
    This data lake could be on cloud storage.

    \section{Data Lakehouse}
    
    \subsection{Challenges with the data lake}
    
    \subsection{Intoduction to the data lakehouse}

    \section{Data Governance with the Data Mesh}
    
    \subsection{Intoduction to Data Mesh}

    \subsection{Data mesh principles: Domain ownership and data as a product}

    \subsection{Data mesh principles: Self service and federated governance}

    \subsection{Data Catalog}

    \subsection{Data Fabric}

    \section{Streaming Data in Data Science}

    \subsection{Introduction to streaming data}
    
    \subsection{Kafka 101} %Hahahahahhaha
    
    \subsection{Lambda architecture}
    
    \subsection{Kapppa architecture and comparison}
    
    \subsection{Word of caution and Resources}

    \section{Data infrastrcture for Machine Learning}

    \subsection{Feature Store}

    \subsection{Vector Database}

    \section{Flowchart and Use case examples}

    \subsection{Data Architecture decision making flowchart}

    \subsection{Use case examples and applying the decision}

\end{document}