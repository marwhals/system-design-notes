%! Author = Marjan
%! Date = 16/06/2025
% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}

% Document
\begin{document}

    \section{Introduction}
    Motivations
    - Create models that have the potential to deliver high impact on an organisations performance
    - Many of these models fail to make an impact because you are unable to connect them to the right sources.
    - Connecting to the right data source is important so that the models train on the right data sets that represent the business problem and thereby improving their accuracy and also removing any bias they might display in production.
    - Data engineers and data architects are actually responsible for ensuring models get access to the right data.
    - The reality though is that they require advice and guidance from the data scientists to ensure this does become a reality
    - Knowing data architecture will also help plan projects in such a way.

    Important to plan ahead.
    Allows for notebook dreams to become highly impactful applications.

    Note: Data architecture is outside the core competency of a data scientists roles and responsibility.

%    TODO - Million dollaaaaah slide. LaTeX or mermaid

    \section{Data Type}

    \subsection{Structured data}
    - Structured data typically constitutes only 20\% of the total data volume and organisation produces or has access to.
    -- Many organisations have reached a reasonable level of maturity in dealing with structured data.
    -- Its important to understand how structured data is managed by your data engineering team.

    Structured data can be found in database tables, CSV files and Excel spreadysssss.
    - Organised into rows and columns.
    - The columns contain the features you need to train your model and the rows contain the observations.
    --- Ultimate goal of a data architect or a data engineering team is to extract structured data from the different sources and load it into the data warehouse.
    ----- Since a data warehouse is supposed to have a very accurate information it is not permitted to load data that is unclean or incomplete.
    ----- Hence structured data is first loaded into staging tables where data engineers perform cleansing operation, such as replacing null values in all the columns.
    ----- Staging tables may also be used to join data from different sources and unify them into a single table.
    ----- They may be used to also enrich existing data with information from an external data source.

    All in all the staging tables serve as a playground for the data engineer to transform the data and make it ready for loading the data into the warehouse.
    The data engineer may choose to retain teh staging tables permanently or just generate them dynamically as needed.

    Generally speaking, the data scientists will not have access to the staging tables.
    Once the data is clean unified and enriched, it is read to be loaded into a data warehouse.

    Very important: A data warehouse represents the single source of truth for the entire organisation.

    Example:
    Management of a company wants to know total sales for a month. The expect that business analysts can make a query to the data warehouse and get the answer.
    The data scientist generally has just read access to the data warehouse and is allowed just conditional access to certain tables for exploring the possibility of finding features for data science use cases.
    Once the features are found the data engineer and not the data scientist transfers the required data into what is called a data mart.
    The data mart contains a copy of the data that is available in the data warehouse and is refreshed either on request or as per a predefined schedule.

    Now, the data scientists usually has full access to the data mart and allowed to do further pre-precessing and manipulation of the contents.
    Once the data scientist trains the models.
    Data marts are again used to store the data that is used for making predictions and the predictions themselves are also stored inside the data mart from where it's picked up by applications or just the dashboarding tools.

    \subsection{Unstructured data}

    \subsection{Semi-structured data}]

    \subsection{Short explaination of JSON and XML structures}
    
    \subsection{Semi-Structured data in machine learninig}

    \section{Data Warehouse}
    
    \subsection{Introduction to Data Warehousing}
    A data warehouse is the single source of truth for the entire organisation.
    A data warehouse is generally used to store information from structured data sources.
    An organisation collects data from its operational databases and organises it inside a data warehouse, in a format you can perform analytics on.
    An operational database is one that holds transactions or data that is written by applications.
    Examples to consider.
    - Could have a web shop which collects its transactions in a sales database.
    - CRM which stores all information in a customer databse.............

    Could perform analytics directly on these operational databases.
    However, this will impact the performance of your applications.
    It will be detrimental to your business if you web shop is running slow, just because some data scientist was querying the data to explore the available features.
    It's also the case that operational database are designed to store transactions and are not easy to query for analytical purposes. (Pretty key point)
    Operational databases are designed to be written to, whereas analytical databases need to provide more read performance.
    Analytical databases perform better when the data is stored in a columnar format, whereas operational databases are designed for storing transactions in rows.

    Very key point.
    Makes a lot of sense for any organisation to extract data from all its operational databases and store it in a data warehouse,
    and store it in a way that makes it easy for querying by analysts and data scientists alike.

    Companies also enrich their data warehouse with data from external sources.
    Combine real time data from the internet with operational data.

    \subsection{Datawarehousing for data scientists}
    
    \subsection{Cloud datawarehousing}

    \section{Data Lake}

    \subsection{Introduction to a data lake}
    
    \subsection{The technology used to build a data lake}
    
    \subsection{Cloud Storage terminology - Buckets and blobs}
    
    \section{Data Lakehouse}
    
    \subsection{Challenges with the data lake}
    
    \subsection{Intoduction to the data lakehouse}

    \section{Data Governance with the Data Mesh}
    
    \subsection{Intoduction to Data Mesh}

    \subsection{Data mesh principles: Domain ownership and data as a product}

    \subsection{Data mesh principles: Self service and federated governance}

    \subsection{Data Catalog}

    \subsection{Data Fabric}

    \section{Streaming Data in Data Science}

    \subsection{Introduction to streaming data}
    
    \subsection{Kafka 101} %Hahahahahhaha
    
    \subsection{Lambda architecture}
    
    \subsection{Kapppa architecture and comparison}
    
    \subsection{Word of caution and Resources}

    \section{Data infrastrcture for Machine Learning}

    \subsection{Feature Store}

    \subsection{Vector Database}

    \section{Flowchart and Use case examples}

    \subsection{Data Architecture decision making flowchart}

    \subsection{Use case examples and applying the decision}

\end{document}